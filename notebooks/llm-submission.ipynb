{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## # üß† Chatbot Arena Preference Prediction with XGBoost","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üìö Step 1: Import Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom scipy.sparse import hstack\nimport xgboost as xgb","metadata":{"execution":{"iopub.status.busy":"2025-03-25T03:43:02.121790Z","iopub.execute_input":"2025-03-25T03:43:02.122319Z","iopub.status.idle":"2025-03-25T03:43:02.129076Z","shell.execute_reply.started":"2025-03-25T03:43:02.122282Z","shell.execute_reply":"2025-03-25T03:43:02.127624Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"## üìÇ Step 2: Load Training Data","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/llm-classification-finetuning/train.csv')","metadata":{"execution":{"iopub.status.busy":"2025-03-25T03:43:02.130848Z","iopub.execute_input":"2025-03-25T03:43:02.131205Z","iopub.status.idle":"2025-03-25T03:43:04.278332Z","shell.execute_reply.started":"2025-03-25T03:43:02.131178Z","shell.execute_reply":"2025-03-25T03:43:04.277169Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"## üßπ Step 3: Preprocess and Clean Text","metadata":{}},{"cell_type":"code","source":"train_df['text_a'] = (train_df['prompt'] + \" \" + train_df['response_a']).str.lower().str.strip()\ntrain_df['text_b'] = (train_df['prompt'] + \" \" + train_df['response_b']).str.lower().str.strip()","metadata":{"execution":{"iopub.status.busy":"2025-03-25T03:43:04.280425Z","iopub.execute_input":"2025-03-25T03:43:04.280828Z","iopub.status.idle":"2025-03-25T03:43:04.648339Z","shell.execute_reply.started":"2025-03-25T03:43:04.280788Z","shell.execute_reply":"2025-03-25T03:43:04.647337Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## üéØ Step 4: Create Target Label","metadata":{}},{"cell_type":"code","source":"def get_label(row):\n    if row['winner_model_a'] == 1:\n        return 'a'\n    elif row['winner_model_b'] == 1:\n        return 'b'\n    else:\n        return 'tie'\n\ntrain_df['label'] = train_df.apply(get_label, axis=1)","metadata":{"execution":{"iopub.status.busy":"2025-03-25T03:43:04.649885Z","iopub.execute_input":"2025-03-25T03:43:04.650238Z","iopub.status.idle":"2025-03-25T03:43:05.224494Z","shell.execute_reply.started":"2025-03-25T03:43:04.650213Z","shell.execute_reply":"2025-03-25T03:43:05.223158Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"## üî† Step 5: TF-IDF Vectorization","metadata":{}},{"cell_type":"code","source":"tfidf = TfidfVectorizer(max_features=5000)\ntfidf.fit(pd.concat([train_df['text_a'], train_df['text_b']]))\n\nX_a = tfidf.transform(train_df['text_a'])\nX_b = tfidf.transform(train_df['text_b'])\nX_concat = hstack([X_a, X_b])","metadata":{"execution":{"iopub.status.busy":"2025-03-25T03:43:05.226053Z","iopub.execute_input":"2025-03-25T03:43:05.226893Z","iopub.status.idle":"2025-03-25T03:44:00.702217Z","shell.execute_reply.started":"2025-03-25T03:43:05.226848Z","shell.execute_reply":"2025-03-25T03:44:00.701305Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## üéØ Step 6: Encode Labels","metadata":{}},{"cell_type":"code","source":"le = LabelEncoder()\ny_encoded = le.fit_transform(train_df['label'])","metadata":{"execution":{"iopub.status.busy":"2025-03-25T03:44:00.703161Z","iopub.execute_input":"2025-03-25T03:44:00.703526Z","iopub.status.idle":"2025-03-25T03:44:00.721530Z","shell.execute_reply.started":"2025-03-25T03:44:00.703485Z","shell.execute_reply":"2025-03-25T03:44:00.720160Z"},"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"## üß™ Step 7: Train/Validation Split","metadata":{}},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(\n    X_concat, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n)","metadata":{"execution":{"iopub.status.busy":"2025-03-25T03:44:00.722812Z","iopub.execute_input":"2025-03-25T03:44:00.723226Z","iopub.status.idle":"2025-03-25T03:44:00.813980Z","shell.execute_reply.started":"2025-03-25T03:44:00.723184Z","shell.execute_reply":"2025-03-25T03:44:00.812908Z"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## üå≤ Step 8: Train XGBoost Model with Early Stopping","metadata":{}},{"cell_type":"code","source":"dtrain = xgb.DMatrix(X_train, label=y_train)\ndval = xgb.DMatrix(X_val, label=y_val)\n\nparams = {\n    'objective': 'multi:softprob',\n    'num_class': 3,\n    'eval_metric': 'mlogloss',\n    'max_depth': 6,\n    'eta': 0.1,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'tree_method': 'hist',\n    'verbosity': 1\n}\n\nbst = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=500,\n    evals=[(dtrain, 'train'), (dval, 'val')],\n    early_stopping_rounds=20,\n    verbose_eval=10\n)","metadata":{"execution":{"iopub.status.busy":"2025-03-25T03:44:00.816710Z","iopub.execute_input":"2025-03-25T03:44:00.817128Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[0]\ttrain-mlogloss:1.09352\tval-mlogloss:1.09508\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## üìä Step 9: Evaluate on Validation Set","metadata":{}},{"cell_type":"code","source":"# ‚úÖ Evaluate model on validation set\ny_val_probs = bst.predict(dval)  # Get predicted probabilities\ny_pred = np.argmax(y_val_probs, axis=1)  # Convert to predicted class indices\n\n# Convert numeric predictions back to class labels\ny_val_labels = le.inverse_transform(y_val)\ny_pred_labels = le.inverse_transform(y_pred)\n\nprint(\"‚úÖ Classification Report:\")\nprint(classification_report(y_val_labels, y_pred_labels))\nprint(\"üß± Confusion Matrix:\")\nprint(confusion_matrix(y_val_labels, y_pred_labels))\nprint(f\"üéØ Accuracy: {accuracy_score(y_val_labels, y_pred_labels):.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üß™ Step 10: Load and Preprocess Test Data","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\ntest_df['text_a'] = (test_df['prompt'] + \" \" + test_df['response_a']).str.lower().str.strip()\ntest_df['text_b'] = (test_df['prompt'] + \" \" + test_df['response_b']).str.lower().str.strip()\n\nX_test_a = tfidf.transform(test_df['text_a'])\nX_test_b = tfidf.transform(test_df['text_b'])\nX_test_concat = hstack([X_test_a, X_test_b])\ndtest = xgb.DMatrix(X_test_concat)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üìù Step 11: Predict and Generate Submission","metadata":{}},{"cell_type":"code","source":"# Get probabilities\ny_test_probs = bst.predict(dtest)  # shape: (num_samples, num_classes)\n\n# Convert to predicted class indices\ny_test_pred = np.argmax(y_test_probs, axis=1)\n\n# Convert to class labels ('a', 'b', 'tie')\ny_test_labels = le.inverse_transform(y_test_pred)\n\nsubmission_df = pd.DataFrame({\n    'id': test_df['id'],\n    'winner_model_a': y_test_probs[:, le.transform(['a'])[0]],\n    'winner_model_b': y_test_probs[:, le.transform(['b'])[0]],\n    'winner_tie': y_test_probs[:, le.transform(['tie'])[0]],\n})\n\nsubmission_df.to_csv('submission.csv', index=False)\nprint(\"‚úÖ submission.csv file is ready for Kaggle upload.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# üîÑ Load and preprocess test data\ntest_df = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\ntest_df['text_a'] = (test_df['prompt'] + \" \" + test_df['response_a']).str.lower().str.strip()\ntest_df['text_b'] = (test_df['prompt'] + \" \" + test_df['response_b']).str.lower().str.strip()\n\nX_a_test = tfidf.transform(test_df['text_a'])\nX_b_test = tfidf.transform(test_df['text_b'])\nX_test_concat = hstack([X_a_test, X_b_test])\n\n# üîç Predict class probabilities\ndtest = xgb.DMatrix(X_test_concat)\ny_test_probs = bst.predict(dtest)  # shape: (num_samples, num_classes)\n\n# üßæ Generate submission file\nsubmission_df = pd.DataFrame({\n    'id': test_df['id'],\n    'winner_model_a': y_test_probs[:, le.transform(['a'])[0]],\n    'winner_model_b': y_test_probs[:, le.transform(['b'])[0]],\n    'winner_tie': y_test_probs[:, le.transform(['tie'])[0]],\n})\n\nsubmission_df.to_csv('submission.csv', index=False)\nprint(\"‚úÖ Submission file saved as submission.csv\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}